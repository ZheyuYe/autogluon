learning:
  early_stopping_patience: 10
  log_metrics: auto
  stop_metric: auto
  valid_ratio: 0.15
misc:
  exp_dir: /Users/xjshi/Documents/GitHub/autogluon/tests/unittests/ag_text/task1
  seed: 123
model:
  backbone:
    name: google_electra_small
  network:
    agg_net:
      activation: tanh
      agg_type: concat
      data_dropout: False
      dropout: 0.1
      initializer:
        bias: ['zeros']
        weight: ['xavier', 'uniform', 'avg', 3.0]
      mid_units: -1
      norm_eps: 1e-05
      normalization: layer_norm
      num_layers: 0
    categorical_net:
      activation: leaky
      data_dropout: False
      dropout: 0.1
      emb_units: 32
      initializer:
        bias: ['zeros']
        embed: ['xavier', 'gaussian', 'in', 1.0]
        weight: ['xavier', 'uniform', 'avg', 3.0]
      mid_units: 64
      norm_eps: 1e-05
      normalization: layer_norm
      num_layers: 1
    feature_units: -1
    initializer:
      bias: ['zeros']
      weight: ['truncnorm', 0, 0.02]
    numerical_net:
      activation: leaky
      data_dropout: False
      dropout: 0.1
      initializer:
        bias: ['zeros']
        weight: ['xavier', 'uniform', 'avg', 3.0]
      input_centering: False
      mid_units: 128
      norm_eps: 1e-05
      normalization: layer_norm
      num_layers: 1
    text_net:
      pool_type: cls
      use_segment_id: True
  preprocess:
    max_length: 128
    merge_text: True
optimization:
  batch_size: 32
  begin_lr: 0.0
  final_lr: 0.0
  layerwise_lr_decay: 0.8
  log_frequency: 0.1
  lr: 5.5e-05
  lr_scheduler: triangular
  max_grad_norm: 1.0
  model_average: 5
  num_train_epochs: 1
  optimizer: adamw
  optimizer_params: [('beta1', 0.9), ('beta2', 0.999), ('epsilon', 1e-06), ('correct_bias', False)]
  per_device_batch_size: 16
  val_batch_size_mult: 2
  valid_frequency: 0.1
  warmup_portion: 0.1
  wd: 0.01
version: 1